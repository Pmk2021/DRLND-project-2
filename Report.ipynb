{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will see an implementation for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation specs\n",
    "\n",
    "### Summary\n",
    "\n",
    "We implement a continuous machine learning algorithm to solve the multi-agent version of the Reacher problem -- controlling mechanical arms to reach a specific positions in space, with state variables describing the arm physical state and action variables controlling the joints.\n",
    "\n",
    "To do so, we implement a [Proximal Policy Optimization (PPO) algorithm](https://openai.com/blog/openai-baselines-ppo/), \n",
    "with a partially shared network for the actor and critic estimators, with a stochastic policy, for any number of identical\n",
    "agents.  We also use [Generalized Advantage Estimation (GAE)](https://arxiv.org/abs/1506.02438) to interpolate trajectory returns for various lengths.  PPO has some fame for being simple to implement, stable, and easy to tune (for a deep RL algorithm), despite being as new as 2017.\n",
    "\n",
    "This implementation is built after reviewing the implementation of [Shangtong Zhang's \n",
    "Deep RL](https://github.com/ShangtongZhang/DeepRL) PPO continuous algorithm in PyTorch -- similar code structure,\n",
    "but with different hyperparameters, loss function weights, and creating an object-oriented representation\n",
    "for the agent itself outside of the training code.\n",
    "\n",
    "### Details\n",
    "\n",
    "#### Policy and values\n",
    "\n",
    "A shared neural network with two heads is responsible for computing a stochastic policy (actor) and value estimation (critic).\n",
    "\n",
    "A basis body of 2 fully connected layers, with rectified linear units, extracts 'features' that are used by the actor\n",
    "and the critic networks:\n",
    "\n",
    "     s -> Linear(|s|, 512) -> ReLU() -> Linear(512, 512) -> ReLU() -> F\n",
    "     \n",
    "The features are then processed by two distinct networks to provide policy and value estimates:\n",
    "\n",
    "    F -> Linear(512, 512) -> ReLU() -> Linear(512, |A|) -> Tanh() -> [\\mu(s, a_i) for a_i in A]\n",
    "    F -> Linear(512, 512) -> ReLU() -> Linear(512, 1) -> V(s)\n",
    "    \n",
    "The policy generates actions stochastically according to a normal distribution, with standard deviation $\\sigma$ also maintained as model parameters:\n",
    "\n",
    "$$ \\pi (s, a_i) \\sim \\mathcal{N}(\\mu(s, a_i), \\sigma^2) $$\n",
    "\n",
    "The log probability of the distribution is also computed from this policy.\n",
    "\n",
    "This logic is implemented under class `GaussianActorCriticNet` under `model.py`.\n",
    "\n",
    "\n",
    "#### Trajectory and batch extraction\n",
    "\n",
    "We maintain trajectories of `trajectory_length` = 1000 episodes over all 20 agents simultaneously, storing experience tuples (s, a, log_prob, v, r, non_terminal):\n",
    "\n",
    "| variable      | description                                                  |\n",
    "|---------------|--------------------------------------------------------------|\n",
    "| s             | Initial state for the experience                             |\n",
    "| a             | Action chosen by the policy                                  |\n",
    "| log_prob      | Log probability for the action to be generated by the policy |\n",
    "| v             | Value for the state computed by the critic                   |\n",
    "| r             | Reward obtained from the action                              |\n",
    "| non_terminal  | Whether this state is non-terminal (1 - done)                |\n",
    "\n",
    "When enough tuples are collected, they are rolled back from end to start, associating and advantage function and a cumulative backward reward for each experience:\n",
    "\n",
    "    returns := rewards + gamma * non_terminals * returns\n",
    "    td_error := rewards + gamma * non_terminals * next_values - values\n",
    "    advantages := advantages * gae_lambda * gamma * non_terminals + td_error\n",
    "    \n",
    "Finally, for a number of `optimization_steps`, minibatches of size `batch_size` are collected (for each of the agents) and used to train the network according to the combined loss function.\n",
    "\n",
    "#### Loss functions\n",
    "\n",
    "For the policy loss function, we use the clipped version of the PPO policy loss, $L_{CLIP}(\\theta)$, computed over the sampled batches:\n",
    "\n",
    "$$ L(\\theta) = \\prod_{\\tau \\in \\mathcal{T}} \\frac{\\pi_{new}(s, a)}{\\pi_{old}(s, a)} A(s, a) = \\exp \\left( \\sum_{\\tau \\in \\mathcal{T}} \\log \\pi_{new}(s,a) - \\sum_{\\tau \\in \\mathcal{T}} \\log \\pi_{old}(s, a) \\right) A(s, a)$$\n",
    "\n",
    "$$ L_{CLIP}(\\theta) = \\min \\left( \\frac{L(\\theta)}{A(s, a)}, \\textrm{clip}\\left(\\frac{L(\\theta)}{A(s, a)}, 1 - \\epsilon, 1 + \\epsilon \\right)A(s, a) \\right) $$\n",
    "\n",
    "For the value loss function, we use mean square error between critic function output and rolled back rewards:\n",
    "\n",
    "$$ L_{\\textrm{value}}(\\theta) = \\sum_s \\left(V(s) - R(s) \\right)^2 $$\n",
    "\n",
    "Finally, we apply a penalty to the entropy of the distribution -- encouraging $\\sigma$ parameters to become smaller / the policy to become less stochastic:\n",
    "\n",
    "$$ E(\\theta) = \\frac{1}{2} \\log (2 \\pi e \\sigma^2) $$\n",
    "\n",
    "The loss function used during optimization is a (weighted) sum of the loss components above:\n",
    "\n",
    "$$ L_{\\textrm{combined}}(\\theta) = L_{CLIP}(\\theta) + w_v L_{\\textrm{value}}(\\theta) + w_e E(\\theta) $$\n",
    "\n",
    "The loss functions are implemented on the agent itself, in `ppo_agent.py`.\n",
    "\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "Agent hyperparameters may be passed as constructor arguments to `MultiAgent`.  The default values, used in this workbook, are:\n",
    "\n",
    "| parameter                | value      | description                                                                   |\n",
    "|--------------------------|------------|-------------------------------------------------------------------------------|\n",
    "| shared_network_units     | [512, 512] | Network topology for shared network between actor and critic functions        | \n",
    "| actor_network_units      | [512]      | Network topology for actor network function                                   |\n",
    "| critic_network_units     | [512]      | Network topology for critic network function                                  |\n",
    "| optimizer_learning_rate  | 5e-4       | Initial learning rate for Adam optimizer                                      |\n",
    "| optimizer_epsilon        | 1e-5       | Tolerance parameter for Adam optimizer                                        |\n",
    "| trajectory_length        | 1000       | Number of steps cached before trajectory rollback                             |\n",
    "| gamma                    | 0.99       | Discount rate for future rewards                                              |\n",
    "| gae_lambda               | 0.9        | Interpolating parameter for GAE                                               |\n",
    "| optimization_steps       | 16         | Number of optimization steps to perform after trajectory rollback             |\n",
    "| batch_size               | 256        | Number of N-agent experiences to collect for a single optimization step       |\n",
    "| gradient_clip            | 0.25       | Clipping parameter for gradient descent during optimization                   |\n",
    "| ppo_ratio_clip_epsilon   | 0.2        | Clipping parameter for the policy loss function                               |\n",
    "| entropy_penalty_weight   | 0.01       | Weight applied to entropy penalty on total loss function                      |\n",
    "| value_loss_weight        | 1.0        | Weifht applied to value loss on total loss function                           |\n",
    "\n",
    "Training hyperparameters are passed on the training function itself, `train_multiagent`, defined below.  The default values are:\n",
    "\n",
    "| parameter                     | value     | description                                           |\n",
    "|-------------------------------|-----------|-------------------------------------------------------|\n",
    "| n_episodes                    | 300       | Maximum number of training episodes                   |\n",
    "| max_t                         | 1000      | Maximum number of steps per episode                   |\n",
    "| solved_score                  | 30        | Average score required to consider problem solved     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Import required system packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the state and action dimensions, and initialize our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "\n",
    "from ppo_agent import MultiAgent\n",
    "\n",
    "multiagent = MultiAgent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    shared_network_units=[512, 512],\n",
    "    actor_network_units=[512],\n",
    "    critic_network_units=[512],\n",
    "    value_loss_weight=1.0,\n",
    "    gradient_clip=0.25,\n",
    "    ppo_ratio_clip_epsilon=0.2,\n",
    "    trajectory_length=1000,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.9,\n",
    "    optimization_steps=16,\n",
    "    batch_size=256,\n",
    "    entropy_penalty_weight=0.01,\n",
    "    optimizer_learning_rate=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define interactions with agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def train_multiagent(\n",
    "    env, \n",
    "    multiagent, \n",
    "    n_episodes=300, \n",
    "    max_t=1000, \n",
    "    display_every=10,\n",
    "    solved_score=30, \n",
    "    save_filename=None\n",
    "):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        n_actors = len(env_info.vector_observations)\n",
    "        score = np.zeros(n_actors)\n",
    "        episode_length_list = []\n",
    "        episode_counter = np.zeros(n_actors)\n",
    "        \n",
    "        for t in range(1, max_t+1):\n",
    "            states = env_info.vector_observations\n",
    "            if np.isnan(states).any():\n",
    "                print('\\nNaN found in states')\n",
    "                return scores\n",
    "            \n",
    "            actions = multiagent.act(states)\n",
    "            \n",
    "            if np.isnan(actions).any():\n",
    "                print('\\nNaN found in actions')\n",
    "                return scores\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = np.array(env_info.rewards)\n",
    "            \n",
    "            if np.isnan(rewards).any():\n",
    "                print('\\nNaN found in rewards')\n",
    "                rewards = np.where(np.isnan(rewards), -5, rewards)\n",
    "            \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            multiagent.step(\n",
    "                states, \n",
    "                actions, \n",
    "                rewards,\n",
    "                next_states, \n",
    "                dones\n",
    "            )\n",
    "            \n",
    "            score += np.array(rewards)\n",
    "            score = np.where(dones, 0, score)\n",
    "\n",
    "        mean_score = score.mean()\n",
    "        scores_window.append(mean_score)       # save most recent score\n",
    "        scores.append(mean_score)              # save most recent score\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage score: {:.2f}'.format(\n",
    "            i_episode, np.mean(scores_window)\n",
    "        ), end=\"\")\n",
    "        if i_episode % display_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.2f}'.format(\n",
    "                i_episode, np.mean(scores_window)\n",
    "            ))\n",
    "            \n",
    "        if np.mean(scores_window) >= solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage score: {:.2f}'.format(\n",
    "                np.maximum(i_episode-100, 0), np.mean(scores_window))\n",
    "             )\n",
    "            if save_filename is not None:\n",
    "                multiagent.save(save_filename)\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent to required scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage score: 0.62\n",
      "Episode 20\tAverage score: 1.21\n",
      "Episode 30\tAverage score: 1.98\n",
      "Episode 40\tAverage score: 2.70\n",
      "Episode 50\tAverage score: 3.45\n",
      "Episode 60\tAverage score: 4.43\n",
      "Episode 70\tAverage score: 6.01\n",
      "Episode 80\tAverage score: 8.03\n",
      "Episode 90\tAverage score: 10.21\n",
      "Episode 100\tAverage score: 12.57\n",
      "Episode 110\tAverage score: 16.17\n",
      "Episode 120\tAverage score: 19.73\n",
      "Episode 130\tAverage score: 23.13\n",
      "Episode 140\tAverage score: 26.39\n",
      "Episode 150\tAverage score: 29.49\n",
      "Episode 152\tAverage score: 30.08\n",
      "Environment solved in 52 episodes!\tAverage score: 30.08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = train_multiagent(env, multiagent, solved_score=30, save_filename=\"checkpoint-reacher-30.pth\")\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now watch our trained agent in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 37.71349915703759\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "multiagent.load('checkpoint-reacher-30.pth')\n",
    "\n",
    "# Run through once with loaded model\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = multiagent.act(states)                   # get actions from model (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "* Experiment with new training environments, implement new training environments\n",
    "* Implement solutions with other algorithms (A2C, ACER, DDPG) and compare performance\n",
    "* Experiment with larger network architectures for faster learning and/or larger score before stabilization\n",
    "* Review literature for PPO extensions\n",
    "* Investigate applicability of meta-learning algorithms to the policy, value, or rewards in the PPO setting, such as:\n",
    "    - [Evolved Policy Gradients](https://arxiv.org/pdf/1802.04821.pdf)\n",
    "    - [Self-Regulated Learning](http://mrl.snu.ac.kr/research/ProjectAerobatics/Aerobatics.htm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
